{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dipendensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'corrections'],\n",
      "        num_rows: 755\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'corrections'],\n",
      "        num_rows: 748\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"jhu-clsp/jfleg\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = data[\"validation\"]\n",
    "test = data[\"test\"]\n",
    "\n",
    "validation_sentence = validation[\"sentence\"]\n",
    "validation_corrections = validation[\"corrections\"]\n",
    "\n",
    "test_sentence = test[\"sentence\"]\n",
    "test_corrections = test[\"corrections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sentence : \n",
      "1, So I think we can not live if old people could not find siences and tecnologies and they did not developped . \n",
      "2, For not use car . \n",
      "3, Here was no promise of morning except that we looked up through the trees we saw how low the forest had swung . \n",
      "4, Thus even today sex is considered as the least important topic in many parts of India . \n",
      "5, image you salf you are wark in factory just to do one thing like pot taire on car if they fire you you will destroy , becouse u dont know more than pot taire in car . \n",
      "\n",
      "\n",
      "Validation corrections : \n",
      "1. 1. So I think we would not be alive if our ancestors did not develop sciences and technologies . \n",
      "1. 2. So I think we could not live if older people did not develop science and technologies . \n",
      "1. 3. So I think we can not live if old people could not find science and technologies and they did not develop . \n",
      "1. 4. So I think we can not live if old people can not find the science and technology that has not been developed . \n",
      "2. 1. Not for use with a car . \n",
      "2. 2. Do not use in the car . \n",
      "2. 3. Car not for use . \n",
      "2. 4. Can not use the car . \n",
      "3. 1. Here was no promise of morning , except that we looked up through the trees , and we saw how low the forest had swung . \n",
      "3. 2. Here , there was no promise of morning , except that we looked up through the trees and saw how low the forest had swung . \n",
      "3. 3. Here was no promise of morning except that we looked up through the trees and we saw how low the forest had swung . \n",
      "3. 4. There was no promise of morning except when we looked up through the trees and saw how low the forest had swung . \n",
      "4. 1. Thus , even today , sex is considered as the least important topic in may parts of India . \n",
      "4. 2. Thus , even today , sex is considered the least important topic in many parts of India . \n",
      "4. 3. Thus , even today , sex is considered the least important topic in many parts of India . \n",
      "4. 4. Thus , even today sex is considered as the least important topic in many parts of India . \n",
      "5. 1. Imagine yourself you are working in factory just to do one thing like put air a on car if they fire you you will be destroyed , because you do n't know more than to put air a in car . \n",
      "5. 2. Imagine that you work in a factory and do just one thing , like put tires on cars ; if they fire you , they will destroy you because you do n't know how to do anything but put tires on cars . \n",
      "5. 3. image you salf you are wark in factory just to do one thing like pot taire on car if they fire you you will destroy , becouse u dont know more than pot taire in car . \n",
      "5. 4. Imagine yourself working in a factory. You are to do just one thing , such as put a tire on a car. If you are fired , it will destroy you because you do not know how to do more than put tires on cars . \n"
     ]
    }
   ],
   "source": [
    "print(\"Validation sentence : \")\n",
    "for index, val_sen in enumerate(validation_sentence[:5], start=1): \n",
    "    print(f\"{index}, {val_sen}\")\n",
    "print(\"\\n\")\n",
    "print(\"Validation corrections : \")\n",
    "for i, val_cor in enumerate(validation_corrections[:5], start=1): \n",
    "    for j, cor in enumerate(val_cor, start=1):\n",
    "        print (f\"{i}. {j}. {cor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pra-pemrosesan Data\n",
    "\n",
    "##### 1.1. Pembersihan Data\n",
    "Data dibersihkan dari karakter yang tidak perlu dibaca, spasi berlebih, dan karakter khusus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    cleaned = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Normalisasi Data\n",
    "Data diubah menjadi huruf kecil dan dikonversikan semua kata menjadi bentuk dasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aditt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aditt\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_kalimat(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = sentence.lower()\n",
    "    kata = sentence.split()\n",
    "    kata_dasar = [lemmatizer.lemmatize(k) for k in kata]\n",
    "    return ' '.join(kata_dasar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Tokenisasi\n",
    "Dilakukan tokenisasi kepada data dengan memecah kalimat ke dalam per-kata, dan dilakukan pemberian id khusus per-kata\n",
    "khusus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aditt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(sentence):\n",
    "    sentence.split()\n",
    "    token_dict = {}\n",
    "    token_list = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict) + 1\n",
    "    \n",
    "    for word in sentence:\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        token_list.extend([(token, token_dict[token]) for token in tokens])\n",
    "    \n",
    "    return np.array(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
